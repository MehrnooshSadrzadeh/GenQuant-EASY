\section{Vector Space and Concrete Corpus  Interpretation}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "Quant"
%%% End: 

In this section, we first provide the computation of the meaning of a quantified phrase in a sentence in vector spaces in general, and then instantiate these in a couple of concrete models and provide corpus-based applications. 

\subsection{Vector Space Computations}

A concrete vector instantiation of the abstract compact closed categorical interpretation is  provided by the tuple $({\cal C}_{W,S}, \ov{\text{\ }})$, where $W$ is a vector space with a basis $\{n_i\}_i$ and $S$ is a vector space with a basis $\{s_j\}_j$.  Vector meanings of words are as follows (the corresponding morphisms are obtained as before): 
\begin{itemize}
\item For a word w with a lexical category N,NP we have  
\[
\overline{\semantics{\text{w}}} := \ov{\text{w}} = \sum_i C_i \ov{n}_i \in W
\]
\item For words w with lexical category VP, we have
\[
\overline{\semantics{\text{w}}} := \ov{\text{w}} = \sum_{ij} C_{ij} \ov{n}_i \otimes \ov{s}_j \in  W \otimes S
\]
\item For words w with lexical category V, we have
\[
\overline{\semantics{\text{w}}} := \ov{\text{w}} =  \sum_{ijk} C_{ijk} \ov{n}_i \otimes \ov{s}_j \otimes \ov{n}_k \in W \otimes S \otimes W
\]
\item For a word d with the lexical category Det, we have 
\[
\overline{\semantics{\text{d}}} := d \colon W \to W \qquad d(\ov{\text {w}})  = \sum_o C_o \ov{n}_o
\]
By linearity of $d$, from the above it follows that 
\[
d(\ov{\text {w}})  =  d(\sum_i  C_i \ov{n}_i) = \sum_i C_i d(\ov{n}_i) = \sum_o C_o \ov{n}_o
\]
So if we take $d(\ov{n}_i) = \sum_t C^i_t \ov{n}_t$, we obtain  that $\sum_{it} C_i C^i_t \ov{n}_t =  \sum_o C_o \ov{n}_o$. 
\end{itemize}

\noindent
In this instantiation, the  meaning of  a sentence with a quantified subject  is obtained by computing the following:

\[
(\epsilon_W \otimes 1_S) \circ (d \otimes  \mu_W \otimes 1_S) \circ (\Delta_W \otimes 1_{W \otimes S} \otimes \epsilon_W)  \circ (\ov{{n}} \otimes \ov{{v}} \otimes \ov{{np}})
\]
Setting $\ov{{n}} = \sum_l C_l \ov{n}_l, \ov{{v}} = \sum_{ijk} C_{ijk} \ov{n}_i \otimes \ov{s}_j \otimes \ov{n}_k$, and $\ov{{np}} = \sum_r C_r \ov{n}_r$, and   unfolding the morphisms, in the first step of the computation we obtain the following (where $\delta_{rk}$ is 1 when $r=k$ and 0 otherwise):

\begin{align*}
(\Delta_W \otimes 1_{W \otimes S} \circ \epsilon_W)\Big(\sum_l C_l \ov{n}_l \otimes \sum_{ijk} C_{ijk} \ov{n}_i \otimes \ov{s}_j \otimes \ov{n}_k \ \otimes \sum_r C_r \ov{n}_r \Big) = \\
(\sum_l C_l \ov{n}_l \otimes \ov{n}_l) \otimes  (\sum_{ijkr} C_{ijk} C_r \ov{n}_i \otimes \ov{s}_j \otimes \langle \ov{n}_k \mid \ov{n}_r \rangle)=\\
(\sum_l C_l \ov{n}_l \otimes \ov{n}_l) \otimes  (\sum_{ijkr} C_{ijk} C_r \ov{n}_i \otimes \ov{s}_j  \delta_{rk})
\end{align*}

\noindent
In the second step we obtain: 

\begin{align*}
(d \otimes  \mu_W \otimes 1_S)\Big((\sum_l C_l \ov{n}_l \otimes \ov{n}_l) \otimes  (\sum_{ijkr} C_{ijk} C_r \ov{n}_i \otimes \ov{s}_j  \delta_{rk})\Big) =\\ 
\sum_{ijkrl} C_{ijk}  C_r  C_l d(\ov{n_l}) \otimes \mu(\ov{n}_l \otimes \ov{n}_i) \otimes \ov{s}_j \delta_{rk}=\\
\sum_{ijkrl} C_{ijk}  C_r  C_l d(\ov{n_l}) \otimes \delta_{li}\ov{n}_i \otimes \ov{s}_j \delta_{rk}
\end{align*}

\noindent
The final step is as follows:

\begin{align*}
(\epsilon_{W} \otimes 1_S) \Big( \sum_{ijkrl} C_{ijk}  C_r  C_l d(\ov{n_l}) \otimes \delta_{li}\ov{n}_i \otimes \ov{s}_j \delta_{rk} \Big) =
\sum_{ijkrl} C_{ijk}  C_r  C_l \langle d(\ov{n_l}) \mid \ov{n}_i \rangle  \delta_{li} \ov{s}_j \delta_{rk}
\end{align*}

\noindent Now if we instantiate $d(\ov{n}_l)$ to $\sum_t C^i_t \ov{n}_t$, the above further simplifies to the following:
 
 \[
\sum_{ijkrl} C_{ijk}  C_r  C_l \langle \sum_t C^i_t \ov{n}_t \mid \ov{n}_i \rangle  \delta_{li} \ov{s}_j \delta_{rk}=
\sum_{ijkrlt} C_{ijk}  C_r  C_l C^i_t \delta_{ti}  \delta_{li} \ov{s}_j \delta_{rk}
\]

\noindent
Similar computations provide us with the following for the meaning of a sentence with a quantified object:
\[
(1_S \otimes \epsilon_W) \circ (1_S \otimes \mu_W \otimes d) \circ (\epsilon_W \otimes 1_{S \otimes W} \otimes \Delta_W)(\ov{{n}} \otimes \ov{{v}} \otimes \ov{{np}}) = \sum_{ijkrlt} C_{ijk} C_r C_l C^i_t \delta_{ri} \ov{s}_j \delta_{kl} \delta_{kt}
\]

\noindent
{\bf Intransitive Example.} 
As an example, take $W$ to be the two dimensional space with the basis $\{\ov{n}_1, \ov{n}_2\}$ and $S$ be the two dimensional space with the basis $\{\ov{s}_1, \ov{s}_2\}$.  Consider an intransitive sentence with a quantified subject and the following linear expansions for its subject and verb:
\[
\ov{np} := C_1 \ov{n}_1 + C_2 \ov{n}_2
\qquad 
 \ov{vp} := C_{11} \ov{n}_1 \otimes \ov{s}_1 + C_{12} \ov{n}_1 \otimes \ov{s}_2 +  C_{21}  \ov{n}_2 \otimes \ov{s}_1 + C_{22} \ov{n}_2 \otimes \ov{s}_2
 \]
    Suppose further the following for the interpretation of the determiner:
 \begin{equation*}\label{eqDetLin}
d(\ov{{np}}) = d(C_1 \ov{n}_1 + C_2 \ov{n}_2) =  C_1 d(\ov{n}_1) + C_2 d(\ov{n}_2) =  C'_1 \ov{n}_1 + C'_2 \ov{n}_2
\end{equation*}
where we further assume the following for the effect of $d$ on each basis:
\[
d(\ov{n}_1) = C^1_1 \ov{n}_1 + C^1_2 \ov{n}_2 \qquad
d(\ov{n}_2) = C^2_1 \ov{n}_1 + C^2_2 \ov{n}_2 
\]
So we obtain the following equivalence between the application of $d$ on words and on basis vectors:
\[
d(\ov{{np}}) = C'_1 \ov{n}_1 + C'_2 \ov{n}_2 \ = \ (C_1 C_1^1 + C_2 C_1^2) \ov{n}_1 + (C_1C_2^1 + C_2C_2^2) \ov{n}_2
\]

In the first step of the computation of the meaning vector of the sentence `Q np vp' we have:
\begin{align*}
(\Delta_W \otimes 1_{W \otimes S})\Big((C_1\ov{n}_1 + C_2 \ov{n}_2) \otimes (C_{11} \ov{n}_1 \otimes \ov{s}_1 + C_{12} \ov{n}_1 \otimes \ov{s}_2
+ C_{21} \ov{n}_2 \otimes \ov{s}_1 + C_{22} \ov{n}_2 \otimes \ov{s}_2) \Big)=\\
(C_1 \ov{n}_1 \otimes \ov{n}_1 + C_2 \ov{n}_2 \otimes \ov{n}_2) \otimes (C_{11} \ov{n}_1 \otimes \ov{s}_1 + C_{12} \ov{n}_1 \otimes \ov{s}_2
+ C_{21} \ov{n}_2 \otimes \ov{s}_1 + C_{22} \ov{n}_2 \otimes \ov{s}_2)
\end{align*}
In the second step of computation, we apply $(d \otimes \mu_W \otimes 1_S)$ to the above and obtain:
\[
(C_1 d(\ov{n}_1) + C_2 d(\ov{n}_2)) \otimes (C_1 C_{11} \ov{n}_1 \otimes \ov{s}_1 +  C_1 C_{12} \ov{n}_1 \otimes \ov{s}_2 + C_2 C_{21} \ov{n}_2 \otimes \ov{s}_1 + C_2 C_{22} \ov{n}_2 \otimes \ov{s}_2) \qquad (*)
\]
In the final step, we apply $(\epsilon_W \otimes 1_S)$ to the above and obtain:
\begin{eqnarray*}
C_1 C_1 C_{11}  \langle d(\ov{n}_1) \mid \ov{n}_1\rangle   \ov{s}_1 +  C_1 C_1 C_{12} \langle d(\ov{n}_1) \mid \ov{n}_1 \rangle  \ov{s}_2 + C_1 C_2 C_{21} \langle d(\ov{n}_1) \mid \ov{n}_2\rangle  \ov{s}_1 + C_1 C_2 C_{22} \langle d(\ov{n}_1) \mid  \ov{n}_2 \rangle  \ov{s}_2 \\
+\\
C_2 C_1 C_{11}  \langle d(\ov{n}_2) \mid \ov{n}_1\rangle   \ov{s}_1 +  C_2 C_1 C_{12} \langle d(\ov{n}_2) \mid \ov{n}_1 \rangle  \ov{s}_2 + C_2 C_2 C_{21} \langle d(\ov{n}_2) \mid \ov{n}_2\rangle  \ov{s}_1 + C_2 C_2 C_{22} \langle d(\ov{n}_2) \mid  \ov{n}_2 \rangle  \ov{s}_2
\end{eqnarray*}
Now,  using the linear expansion of $d$, the above is further simplified   to the following:
\begin{eqnarray*}
C_1 C_1 C_{11}C_1^1 \ov{s}_1 + C_1 C_1 C_{12} C_1^1 \ov{s}_2 + C_1 C_2 C_{21} C_2^1 \ov{s}_1 + C_1 C_2 C_{22} C_2^1 \ov{s}_2\\
+\\
C_2C_1C_{11} C_1^2\ov{s}_1 + C_2C_1C_{12} C_1^2 \ov{s}_2 + C_2C_2C_{21} C_2^2 \ov{s}_1 +
C_2 C_2 C_{22} C_2^2 \ov{s}_2
\end{eqnarray*}

To obtain a more readable notation,  instead of expanding, which is what we have been doing so far, let us factor things out a bit.  First note that the $(*)$ above is equivalent to the following by linearity of the map $d$:
\[
d(C_1 \ov{n}_1 + C_2 \ov{n}_2) \otimes (C_1 C_{11} \ov{n}_1 \otimes \ov{s}_1 +  C_1 C_{12} \ov{n}_1 \otimes \ov{s}_2 + C_2 C_{21} \ov{n}_2 \otimes \ov{s}_1 + C_2 C_{22} \ov{n}_2 \otimes \ov{s}_2) 
\]
Then, using a matrix notation where we assume column vectors are elements of $W$ and 2 by 2 matrices elements of $W \otimes S$, observe that the above can be written down as follows:
\[
\left(
 \left ( \begin{array}{cc}
 C_1 & C_1\\ C_2 & C_2
  \end{array} \right)
 \ \odot \ 
 \left ( \begin{array}{cc}
 C_{11} & C_{12}\\ C_{21} & C_{22}
\end{array} \right ) \right)
 \quad \times \quad
 d\left ( \begin{array}{c} C_1 \\ C_2 \end{array} \right )
 \hspace{2cm} (**)
\]
Here the map $d$ is being applied to the vector meaning of the word $np$  rather to the basis vectors of the vector space $W$. In the preceding computations, the map $d$ was being applied to the basis vectors and the result of the final step of the computation was expressed in that form.  A routine computations shows that the equivalence between the above assumptions on the applications of the $d$, on words or basis vectors,   in linear expansion form has the following matrix form:

\[
d\left ( \begin{array}{c} C_1 \\ C_2 \end{array} \right ) \ = \ \left ( \begin{array}{c} C'_1 \\ C'_2 \end{array} \right ) \ = \ 
\left( \begin{array}{c}
C_1C_1^1 + C_2 C_1^2\\
C_1C_2^1 + C_2C_2^2
\end{array}\right)
\]
This can then be replaces and used in the $(**)$ formula. 


\smallskip
\noindent
{\bf Transitive Example.}
Consider the same two dimensional $W$ and $S$ spaces for the case of transitive sentences and the same assumption for the vector meaning of the subject $\ov{n}$. Assume further that for the vector of the object we have $\ov{np} = C'_1 \ov{n}_1 + C'_2 \ov{n}_2$. In this case, because the verb is an element of a rank 3 tensor space, that is $\ov{v} \in W \otimes S \otimes W$, it is not possible to express it as a matrix in two dimensions: indeed elements of  rank 3 tensor   spaces are cubes rather than matrices. But opting for not expanding the corresponding tensor and just denoting the cube of the verb by $\ov{v}$, the meaning vector of the transitive sentence with a quantified subject can be expressed as follows
\[
\left(
 \left ( \begin{array}{cc}
 C_1 & C_1\\ C_2 & C_2
  \end{array} \right)
 \ \odot \ 
\epsilon (\ov{v} \otimes\left (\begin{array}{c} C'_1 \\C'_2 \end{array} \right) ) \right)
 \quad \times \quad
 d\left ( \begin{array}{c} C_1 \\ C_2 \end{array} \right )
\]
The details of the tedious computations are as before, with the exception that in this case, first the verb has to be applied to its object. This is denoted by the application of the $\epsilon$ map to $\ov{v}$ and matrix form of $\ov{np}$. The rest of the computation is as before: one takes the point wise multiplication of this result with a form of diagonalisation of the vector of the subject, then applies this to the effect of the quantifier map $d$ on the vector of the subject. The result is a sentence vectors in $S$. 

\subsection{Instantiating the map $d$} 

For $\ov{n}_i$ a basis vector of $N$, we instantiate  the map $d$  as follows:

\begin{equation}\label{eqDet}
Det(\ov{n}_i) = \Phi \{\ov{w} \in N \mid d(\ov{n}_i, \ov{w}) = \alpha\}
\end{equation}

\noindent
where we have:
\begin{itemize}
\item  $\phi$ is a linear average function such as the   arithmetic or weighted mean. 
\item  $\alpha$ indicates how close $\ov{w}$ is to the $\ov{n}_i$ and depends on the quantifier expressed by $Det$. 
\end{itemize}

\noindent
The intuitive reading of the above is that $Det$ of a word $\ov{n}_i$ is a linear combination, e.g.  average,  of all the words that are $\alpha$-close to $\ov{n}_i$. In other words, the average of all the words  whose distance from $\ov{n}_i$ is $\alpha$.  For instance, if $Det$ is `few', then $\alpha$ is a small number (closer to 0 than to 1), indicating that we are taking the average of vectors that are not so close to $\ov{n}_i$. If $Det$ is `most', then $\alpha$ will be a large number (closer to 1 than to 0), indicating that we are taking the average of vectors that are close to $\ov{n}_i$. The distance $\alpha$ can be learnt from a corpus using a relevant task. This will extend to any other (non-basis) word by linearity.  

The underlying idea here is that the quantitative way of quantifying in set-theoretic models, which depends on the cardinality of the quantified sets, is now transformed into a geometric way of quantifying where the meaning of the quantified phrase depends on its geometric distance with other words. Hence, a quantified phrase such as `few cats' returns a representative noun (obtained by taking the average of all such nouns) that is far from vector of  `cat'  in the semantic space. This representative noun shares `few' properties with `cat'. A quantified phrase such as `most cats' returns a representative noun that is close the the vector of `cat' and stands for a noun that shares `most' of the properties of `cat'. 

\subsection{Small Corpus-Based Witness.} 

A large scale experimentation  for this model constitutes work in progress.  For the sake of providing  intuitions for the above symbolic constructions, we provide a couple of corpus-based witnesses here. In the distributional models, the most natural instantiation of the distance $d$  in equation \ref{eqDet} is the  co-occurrence distance. For a noun `\emph{n}' and determiners `\emph{few}' and `\emph{most}', we  define these as generally as follows:

\begin{center}
few(\emph{n}) = $Avg\{\mbox{nouns that share  few properties with \emph{n}}\}$\\
most(\emph{n}) = $Avg\{\mbox{nouns that share most properties  with \emph{n}}\}$
\end{center}

For the purpose of this toy-example, the above can be instantiated in the simplest possible way as follows:
\begin{center}
few(\emph{n}) = $Avg\{\mbox{nouns that co-occurred with  \emph{n} few times}\}$\\
most(\emph{n}) = $Avg\{\mbox{nouns that co-occurred with \emph{n} most times}\}$
\end{center}
In this case, a sample query from the online  \emph{Reuter News Corpus}, with at most 100 outputs per query,  provides the following instantiations:
\begin{center}
few(\emph{dogs}) = $Avg\{\mbox{bike, drum, snails}\}$\\
most(\emph{dogs}) = $Avg\{\mbox{cats, pets,birds, puppies}\}$\\
few(\emph{cats}) = $Avg\{\mbox{fluid, needle, care}\}$\\
most(\emph{cats}) = $Avg\{\mbox{dogs, birds, rats, feces}\}$\\
\end{center}
A cosine-based similarity measure over this corpus results in the fact that any of the words in the `most(\emph{n})' set are   more similar to `\emph{n}' than any of  the  words in the `few(\emph{n})' set. This is indeed because  the words in the former set are geometrically closer to `\emph{n}' than the words in the latter set, since they have co-occurred with them more.  This is the first advantage of our model over  a distributional model, where words such as `few' and `most' are treated as  {noise} and hence meanings of phrase such as `few cats', `most cats', and `cats'  become  identical (and similarly for any other noun). Moreover, in our setting we can establish that `most cats' and `most dogs' have similar meanings, because of the over lap of their determiner sets. A larger corpus and a more thorough statistical analysis  will let us achieve more,  that for instance,   `few cats' and `few dogs' also have similar meanings. 

At the level of sentence meanings,  compositional  distributional models  do not interpret determiners (e.g. see  the model of \cite{ML}). As a result, meanings of sentences such as `cats sleep', `most cats sleep' and `few cats sleep'  will become identical; meanings of sentences `most cats sleep' and `few dogs snooze' become very close, since `cats' and `dogs' often occur in the same context and so do `sleep' and `snooze'. In our setting, equation \ref{eqSent} tells us that these sentences have different meanings, since their quantified subjects have different meanings. To see this, take  $\ov{\text{cats}} = C_1 \ov{n}_1 + C_2 \ov{n}_2$, where as $few(\text{cats}) = C'_1 \ov{n}_1 + C'_2 \ov{n}_2$ and $most(\text{cats}) = C''_1 \ov{n}_1 + C''_2 \ov{n}_2$. Instantiating these in equation \ref{eqSent} provides us with the following three different vectors:
\begin{eqnarray*}
\ov{\mbox{cats sleep}} &=&C_1 C_{11} \ov{s}_1 + C_1 C_{12} \ov{s}_2 + C_2 C_{21} \ov{s}_2 + C_2 C_{22} \ov{s}_2\\
\ov{\mbox{few cats sleep}} &=&C'_1 C_{11} \ov{s}_1 + C'_1 C_{12} \ov{s}_2 + C'_2 C_{21} \ov{s}_2 + C'_2 C_{22} \ov{s}_2\\
\ov{\mbox{most cats sleep}} &=& C''_1 C_{11} \ov{s}_1 + C''_1 C_{12} \ov{s}_2 + C''_2 C_{21} \ov{s}_2 + C''_2 C_{22} \ov{s}_2
\end{eqnarray*}
On the other hand, we have  that `most cats sleep' and `most dogs snooze'  have close meanings, one which is close to `pets sleep'.  This is because, their quantified   subjects and their  verbs have similar meanings, that is we have:
\[
\begin{cases}
most(\ov{\text{dogs}}) \sim most(\ov{\text{cats}}) \sim  \ov{\text{pets}}&\\
\ov{\text{snooze}}  \sim \ov{\text{sleep}} &
\end{cases} \implies 
\mbox{most cats sleep}  \sim  \mbox{most dogs snooze} \sim
\mbox{pets sleep}
\]
At the same time,  `few cats sleep' and `most dogs snooze' have a less-close meaning, since their quantified  subjects have different meanings, that is:
\[
most(\ov{\text{dogs}}) \sim\!\!\!\!\!/ \ few(\ov{\text{cats}})  \implies \ov{\mbox{most dogs snooze}}  \sim\!\!\!\!\!/ \
\ov{\mbox{few cats sleep}}
\]


%Suppose $\ov{n}_1$ is the word  `prison' and $\ov{n}_2$ is the word `owner', then one will have the following vectors for the words `cats' and `murderer':
%\[
%\ov{\text{cat}} = 0.8 \  \ov{\text{owner}} + 0.2 \ \ov{\text{prison}}
%\qquad
%\ov{\text{murderer}} = 0.1\  \ov{\text{owner}} + 0.9 \  \ov{\text{prison}}
%\]
%Hence,  meaning of the phrase   `most cats' will  be a word whose vectors is close to the word `cat' in this space, for example `kitten' or `dog', whereas the meaning of `few cats' will be a word whose vectors is far from the word `cat', for example, `murderer'.  Meaning of the sentence `most cats sneeze' will be close to the meaning of the sentence `kittens sneeze', and  meaning of the sentence `few cats sneeze' will be close to  the meaning of the sentence `murderes sneeze'.  In the first case, `most cats' is represented by `kittens' which shares most of the properties of `cats', whereas in the second case, `few cats' is represented by `murderers' which shares very few properties with `cats'. 




