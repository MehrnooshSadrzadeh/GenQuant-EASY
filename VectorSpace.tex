\section{Vector Space Interpretation}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "Quant"
%%% End: 

In a concrete vector space model, built from a corpus using distributional methods, we assume that  vector meaning of the subject is $\sum_i C_i \ov{n}_i \in N$ and the linear map corresponding to the verb is $\sum_{jk} C_{jk} \ov{n}_j \otimes \ov{s}_k \in N \otimes S$.  For $\ov{n}_i$ a basis vector of $N$, we define  the map $Det$  as follows:

\begin{equation}\label{eqDet}
Det(\ov{n}_i) = \Phi \{\ov{w} \in N \mid d(\ov{n}_i, \ov{w}) = \alpha\}
\end{equation}

\noindent
where we have:
\begin{itemize}
\item  $\phi$ is a linear average function such as the   arithmetic or weighted mean. 
\item  $\alpha$ indicates how close $\ov{w}$ is to the $\ov{n}_i$ and depends on the quantifier expressed by $Det$. 
\end{itemize}

\noindent
The intuitive reading of the above is that $Det$ of a word $\ov{n}_i$ is a linear combination, e.g.  average,  of all the words that are $\alpha$-close to $\ov{n}_i$. In other words, the average of all the words  whose distance from $\ov{n}_i$ is $\alpha$.  For instance, if $Det$ is `few', then $\alpha$ is a small number (closer to 0 than to 1), indicating that we are taking the average of vectors that are not so close to $\ov{n}_i$. If $Det$ is `most', then $\alpha$ will be a large number (closer to 1 than to 0), indicating that we are taking the average of vectors that are close to $\ov{n}_i$. The distance $\alpha$ can be learnt from a corpus using a relevant task. This will extend to any other (non-basis) word by linearity.  

The underlying idea here is that the quantitative way of quantifying in set-theoretic models, which depends on the cardinality of the quantified sets, is now transformed into a geometric way of quantifying where the meaning of the quantified phrase depends on its geometric distance with other words. Hence, a quantified phrase such as `few cats' returns a representative noun (obtained by taking the average of all such nouns) that is far from vector of  `cat'  in the semantic space. This representative noun shares `few' properties with `cat'. A quantified phrase such as `most cats' returns a representative noun that is close the the vector of `cat' and stands for a noun that shares `most' of the properties of `cat'. 


With this instantiation, the  meaning of ``Q Sbj Verb'' is obtained by computing the following:

\[
(\epsilon_N \otimes 1_S) \circ (Det \otimes  \mu_N \otimes 1_S) \circ (\delta_N \otimes 1_{N \otimes S})\left(\ov{\text{Sbj}} \otimes \ov{\text{Verb}}\right)
\]
In the first step of computation we have:

\begin{align*}
(\delta_N \otimes 1_{N \otimes S})\Big(\sum_i C_i \ov{n}_i \otimes \sum_{jk} C_{jk} \ov{n}_j \otimes \ov{s}_k\Big) = 
(\sum_i C_i \ov{n}_i \otimes \ov{n}_i) \otimes  (\sum_{jk} C_{jk} \ov{n}_j \otimes \ov{s}_k)
\end{align*}

\noindent
In the second step we obtain: 

\begin{align*}
(Det \otimes  \mu_N \otimes 1_S)\Big((\sum_i C_i \ov{n}_i \otimes \ov{n}_i) \otimes  (\sum_{jk} C_{jk} \ov{n}_j \otimes \ov{s}_k)\Big) &= 
\sum_{ijk} C_i C_{jk} Det(\ov{n}_i) \otimes \mu(\ov{n}_i \otimes \ov{n}_j) \otimes \ov{s}_k \\
= \sum_{ijk} C_i C_{jk} Det(\ov{n}_i) \otimes \delta_{ij} \ov{n}_i \otimes \ov{s}_k&
\end{align*}

\noindent
The final step is as follows:

\begin{align*}
(\epsilon_{N} \otimes 1_S)  \Big(\sum_{ijk} C_i C_{jk} Det(\ov{n}_i) \otimes \delta_{ij} \ov{n}_i \otimes \ov{s}_k \Big) &=   
\sum_{ijk} C_i C_{jk} \langle Det(\ov{n}_i) \mid \delta_{ij} \ov{n}_i \rangle \ov{s}_k
\end{align*}

\noindent
{\bf Example.} 
As a distributional example, take $N$ to be the two dimensional space with the basis $\{\ov{n}_1, \ov{n}_2\}$ and $S$ be the two dimensional space with the basis $\{\ov{s}_1, \ov{s}_2\}$.  Suppose the linear expansion of $\ov{\text{Sbj}}$ in this space is $C_1 \ov{n}_1 + C_2 \ov{n}_2$ and the linear expansion of $\ov{\text{Verb}}$ is $C_{11} (\ov{n}_1 \otimes \ov{s}_1) + C_{12} (\ov{n}_1 \otimes \ov{s}_2) +  C_{21} ( \ov{n}_2 \otimes \ov{s}_1) + C_{22} (\ov{n}_2 \otimes \ov{s}_2)$.    Suppose further the following for the interpretation of the determiner:
 \begin{equation}\label{eqDetLin}
Det(\ov{\text{Sbj}}) = Det(C_1 \ov{n}_1 + C_2 \ov{n}_2) =  Det(C_1 \ov{n}_1) + Det(C_2 \ov{n}_2) =  C'_1 \ov{n}_1 + C'_2 \ov{n}_2
\end{equation}
Then the result of the first step of the computation of a meaning vector for the sentence `Q Sbj Verb' is:
\[
(C_1\ov{n}_1 + C_2 \ov{n}_2) \otimes (C_{11} \ov{n}_1 \otimes \ov{s}_1 + C_{12} \ov{n}_1 \otimes \ov{s}_2
+ C_{21} \ov{n}_2 \otimes \ov{s}_1 + C_{22} \ov{n}_2 \otimes \ov{s}_2)
\]
In the second step of computation we obtain:
\[
C_1C_{11} Det(\ov{n}_1) \ov{n}_1 \otimes \ov{s}_1  + C_1 C_{12} Det(\ov{n}_1) \ov{n}_1 \otimes \ov{s}_2  + C_2 C_{21} Det(\ov{n}_2) \ov{n}_2 \otimes \ov{s}_1 + C_2 C_{22} Det(\ov{n}_2) \ov{n}_2 \otimes \ov{s}_2
\]
Since $Det$ is a linear map, the above is equal to the following:
\[
Det(C_1\ov{n}_1) (C_{11} \ov{n}_1 \otimes \ov{s}_1 + C_{12} \ov{n}_1 \otimes \ov{s}_2) + Det(C_2 \ov{n}_2)(C_{21} \ov{n}_2 \otimes \ov{s}_1 + C_{22} \ov{n}_2 \otimes \ov{s}_2)
\]
According to the expansion of the assumption in equation \ref{eqDetLin},  the above is equivalent to the following:
\[
(C'_1\ov{n}_1) (C_{11} \ov{n}_1 \otimes \ov{s}_1 + C_{12} \ov{n}_1 \otimes \ov{s}_2) + (C'_2 \ov{n}_2)(C_{21} \ov{n}_2 \otimes \ov{s}_1 + C_{22} \ov{n}_2 \otimes \ov{s}_2)
\]
Substituting  this in the last step of the computation provide us with the following vector in $S$:
\begin{equation}\label{eqSent}
C'_1 C_{11} \ov{s}_1 + C'_1 C_{12} \ov{s}_2 + C'_2 C_{21} \ov{s}_2 + C'_2 C_{22} \ov{s}_2
\end{equation}
%Assuming that the basis is fixed, the above can be written in matrix notation as follows:
%\begin{equation}\label{eqSent}
%\left ( \begin{array}{cc} C_{11} & C_{21} \\ C_{21} & C_{22} \end{array} \right) \times 
%\left ( \begin{array}{c} C'_1 \\ C'_2 \end{array} \right) \ = \ \ov{\text{Verb}} \times Det(\ov{\text{Sbj}})
%\end{equation}
%

\noindent
{\bf Small Corpus-Based Witness.} 
A large scale experimentation  for this model constitutes work in progress.  For the sake of providing  intuitions for the above symbolic constructions, we provide a couple of corpus-based witnesses here. In the distributional models, the most natural instantiation of the distance $d$  in equation \ref{eqDet} is the  co-occurrence distance. For a noun `\emph{n}' and determiners `\emph{few}' and `\emph{most}', we  define these as follows:
\begin{center}
few(\emph{n}) = $Avg\{\mbox{nouns that co-occurred with  \emph{n} few times}\}$\\
most(\emph{n}) = $Avg\{\mbox{nouns that co-occurred with \emph{n} most times}\}$
\end{center}
In this case, a sample query from the online  \emph{Reuter News Corpus}, with at most 100 outputs per query,  provides the following instantiations:
\begin{center}
few(\emph{dogs}) = $Avg\{\mbox{bike, drum, snails}\}$\\
most(\emph{dogs}) = $Avg\{\mbox{cats, pets,birds, puppies}\}$\\
few(\emph{cats}) = $Avg\{\mbox{fluid, needle, care}\}$\\
most(\emph{cats}) = $Avg\{\mbox{dogs, birds, rats, feces}\}$\\
\end{center}
A cosine-based similarity measure over this corpus results in the fact that any of the words in the `most(\emph{n})' set are   more similar to `\emph{n}' than any of  the  words in the `few(\emph{n})' set. This is indeed because  the words in the former set are geometrically closer to `\emph{n}' than the words in the latter set, since they have co-occurred with them more.  This is the first advantage of our model over  a distributional model, where words such as `few' and `most' are treated as  {noise} and hence meanings of phrase such as `few cats', `most cats', and `cats'  become  identical (and similarly for any other noun). Moreover, in our setting we can establish that `most cats' and `most dogs' have similar meanings, because of the over lap of their determiner sets. A larger corpus and a more thorough statistical analysis  will let us achieve more,  that for instance,   `few cats' and `few dogs' also have similar meanings. 

At the level of sentence meanings,  compositional  distributional models  do not interpret determiners (e.g. see  the model of \cite{ML}). As a result, meanings of sentences such as `cats sleep', `most cats sleep' and `few cats sleep'  will become identical; meanings of sentences `most cats sleep' and `few dogs snooze' become very close, since `cats' and `dogs' often occur in the same context and so do `sleep' and `snooze'. In our setting, equation \ref{eqSent} tells us that these sentences have different meanings, since their quantified subjects have different meanings. To see this, take  $\ov{\text{cats}} = C_1 \ov{n}_1 + C_2 \ov{n}_2$, where as $few(\text{cats}) = C'_1 \ov{n}_1 + C'_2 \ov{n}_2$ and $most(\text{cats}) = C''_1 \ov{n}_1 + C''_2 \ov{n}_2$. Instantiating these in equation \ref{eqSent} provides us with the following three different vectors:
\begin{eqnarray*}
\ov{\mbox{cats sleep}} &=&C_1 C_{11} \ov{s}_1 + C_1 C_{12} \ov{s}_2 + C_2 C_{21} \ov{s}_2 + C_2 C_{22} \ov{s}_2\\
\ov{\mbox{few cats sleep}} &=&C'_1 C_{11} \ov{s}_1 + C'_1 C_{12} \ov{s}_2 + C'_2 C_{21} \ov{s}_2 + C'_2 C_{22} \ov{s}_2\\
\ov{\mbox{most cats sleep}} &=& C''_1 C_{11} \ov{s}_1 + C''_1 C_{12} \ov{s}_2 + C''_2 C_{21} \ov{s}_2 + C''_2 C_{22} \ov{s}_2
\end{eqnarray*}
On the other hand, we have  that `most cats sleep' and `most dogs snooze'  have close meanings, one which is close to `pets sleep'.  This is because, their quantified   subjects and their  verbs have similar meanings, that is we have:
\[
\begin{cases}
most(\ov{\text{dogs}}) \sim most(\ov{\text{cats}}) \sim  \ov{\text{pets}}&\\
\ov{\text{snooze}}  \sim \ov{\text{sleep}} &
\end{cases} \implies 
\mbox{most cats sleep}  \sim  \mbox{most dogs snooze} \sim
\mbox{pets sleep}
\]
At the same time,  `few cats sleep' and `most dogs snooze' have a less-close meaning, since their quantified  subjects have different meanings, that is:
\[
most(\ov{\text{dogs}}) \sim\!\!\!\!\!/ \ few(\ov{\text{cats}})  \implies \ov{\mbox{most dogs snooze}}  \sim\!\!\!\!\!/ \
\ov{\mbox{few cats sleep}}
\]


%Suppose $\ov{n}_1$ is the word  `prison' and $\ov{n}_2$ is the word `owner', then one will have the following vectors for the words `cats' and `murderer':
%\[
%\ov{\text{cat}} = 0.8 \  \ov{\text{owner}} + 0.2 \ \ov{\text{prison}}
%\qquad
%\ov{\text{murderer}} = 0.1\  \ov{\text{owner}} + 0.9 \  \ov{\text{prison}}
%\]
%Hence,  meaning of the phrase   `most cats' will  be a word whose vectors is close to the word `cat' in this space, for example `kitten' or `dog', whereas the meaning of `few cats' will be a word whose vectors is far from the word `cat', for example, `murderer'.  Meaning of the sentence `most cats sneeze' will be close to the meaning of the sentence `kittens sneeze', and  meaning of the sentence `few cats sneeze' will be close to  the meaning of the sentence `murderes sneeze'.  In the first case, `most cats' is represented by `kittens' which shares most of the properties of `cats', whereas in the second case, `few cats' is represented by `murderers' which shares very few properties with `cats'. 




