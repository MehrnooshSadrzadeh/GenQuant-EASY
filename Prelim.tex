
\section{Preliminaries}
\label{prelim}

\subsection{Corpus-Based Vector Space models of Natural Language}
\label{DistVect}

These models are also referred to by \emph{distributional} models. The idea behind them mainly originates from the work of  Firth \cite{Firth} who argued that the meaning of a word depends on the context in which it often occurs. Based on this idea, computational linguists developed a vector space model for word meanings \cite{???}. We denote such a model with a tuple  $(V_{l[\Sigma]}, \ov{\text{\ }})$, described below:
\begin{itemize}
\item $l\subseteq \Sigma \times \Sigma$ is a relation on the vocabulary $\Sigma$ of the language: for each $w \in \Sigma$, we have that $l(w)$ is the canonical form, otherwise known as the lemma, corresponding to the word $w$.   Hence $l[\Sigma]$, which is the image of $l$ on $\Sigma$ is the set of lemmas of the vocabulary $\Sigma$ of a  language.  An example of a lemma is `kill':  the canonical form of  words such as `kill', `kills', `killed', `killing',  etc. 
\item $V_{l(\Sigma)}$ is a vector space whose basis vectors are the lemmas of the language; we refer to each such basis vectors  by $\ov{v}_i$. 
\item The meaning of each word $w$ is represented by a vector $\ov{w} = \sum_i C_i \ov{v}_i$, obtained by counting and normalising the number of times $w$ has occurred in the context window of any of the  words in $l^{-1}({v}_i)$.   We refer to each such $C_i$ by `the degree to which $w$ has co-occurred with $\ov{v}_i$. The length of the context window and the normalisation scheme are usually treated as parameters of the model.
\end{itemize}

Here is an example of such a model:

\begin{figure}[h]
\begin{center}
\begin{minipage}{10cm}
\begin{center}
\setlength{\unitlength}{0.7mm}
\begin{picture}(60, 70)
  \linethickness{0.3mm}
  \put(26,57){\text{\bf food}}
  \put(30, 20){\vector(1, 0){45}}
  \put(77,18){\text{\bf owner}}
  \put(30, 20){\vector(0, 1){35}}
  \put(30, 20){\vector(-1, -1){25}}
  \put(-4,-12){\text{\bf pillow}}
   
  \linethickness{0.3mm}
   
  \put(30, 20){\begin{color}{blue}\vector(1, 1){18}\end{color}}
  \put(42,48){\text{\bf \begin{color}{blue}cat\end{color}}}
  
  \put(30, 20){\begin{color}{blue}\vector(1, 2){12}\end{color}}
  \put(50,40){\text{\bf \begin{color}{blue}dog\end{color}}}
  
  \put(30, 20){\begin{color}{red}\vector(-2, -1){25}\end{color}}
  \put(-5,15){\text{\bf \begin{color}{red}sleep\end{color}}}
  
  \put(30, 20){\begin{color}{red}\vector(-4, -1){25}\end{color}}
  \put(-10,6){\text{\bf \begin{color}{red}snooze\end{color}}}
%  \put(30, 20){\begin{color}{magenta}\vector(-1, 2){14}\end{color}}
%  \put(-17,50){\text{\bf \begin{color}{magenta}gaze $\otimes$ gesture\end{color}}}
 \end{picture}
\end{center}

\vspace{0.9cm}
\end{minipage}
\end{center}
\caption{An example of a  vector space model of meaning}
\label{fig:VectMod}
\end{figure}

In Figure \ref{fig:VectMod}, for instance for $n=5$, the coordinate of cat on food is the normalised count of the number of times cat has occurred 5 words before or after any  word whose lemma is food. A normalisation scheme is to divide the number of occurrences by the total number of times a word has occurred in a corpus. 



The most mainstream  application of distributional models has been to deciding about the degree of similarity between meanings  of words \cite{Curran}. The cosine of the angle of the vector representations of words has proven to be a good measure of such similarity decisions. This is backed by the  philosophy behind these models, which implies that words that have similar meanings often occur in the same context. 
For instance in the above example, we have a high degree of similarity between meanings of words  cat and dog and less so between dog and snooze. 



\subsection{Generalised Quantifier Theory in Natural Language}
\label{GenQuant}

We briefly review the theory of generalised quantifiers in natural language \cite{BarwiseCooper81}.   The definitions and concepts are slightly revised from their original presentation so that they could fit easier  within the categorical setting, introduced later on. 
Given  a vocabulary $\Sigma$ and a set of lexical categories $S$, the words of the vocabulary are assigned lexical categories via the relation ${\cal X} \subseteq \Sigma \times S$, otherwise known as a lexicon or a dictionary.  The expressions of the language are generated over these categories via a  set of  rules ${\cal R} \subseteq S \times S \times S$.  We refer to a language so defined  by the tuple ${\cal L}_{\Sigma} = ({\cal X}_S, {\cal R})$. 


For the purpose of this paper, we consider the fragment of English generated over the following lexicon:

\begin{center}
\begin{tabular}{c|c}
Syntactic Category & Vocabulary\\
\hline
NP &  John, Mary, something, $\cdots$\\
N&   cat, dog, man, $\cdots$\\
VP &  sneeze, sleep,$\cdots$\\
V &   love, kiss, $\cdots$\\
Det &  a, the, some, every, each, all, no, most, few, one, two, $\cdots$ 
\end{tabular}
\end{center}

\noindent
with the   following phrase structure description rules:

\begin{center}
\begin{tabular}{ccc}
S & $\to$ & NP VP \\
VP & $\to$ & V NP \\
NP &$\to$ & Det N \\
\end{tabular}
\end{center}


\noindent
A model for this language is a pair $(U, \semantics{\ })$, where $U$ is a universal reference set and $\semantics{\ }$ is a function that assigns interpretations to the expressions of  language. The  interpretations   of basic expressions  are  as follows:  

\begin{eqnarray*}
np \in \text{NP}  &\quad \implies \quad &  \semantics{np}   \in U   \\
n \in \text{N} &\quad \implies \quad& \semantics{n}  \subseteq U \\
vp \in  \text{VP} &\quad \implies \quad& \semantics{vp}  \subseteq U \\
v \in \text{V} &\quad \implies \quad& \semantics{v} \subseteq U \times U   \\
d \in \text{Det} &\quad \implies \quad& \semantics{d} \colon {\cal P}(U) \to {\cal P}{\cal P}(U)
\end{eqnarray*}


\noindent
Noun phrases are interpreted as elements of the reference set, nouns and verb phrases as unary relations over it, and verbs as binary relations over it.  Determiners are interpreted as \emph{generalised quantifier};  a  generalised quantifier $d$  is a map that  assigns to each $A \subseteq U$, a family of subsets of $U$; examples are as follows:

\begin{eqnarray*}
\semantics{\text some}(A) &=& \{X \subseteq U \mid X \cap A \neq \emptyset\}\\
\semantics{\text Every}(A) &=& \{X \subseteq U \mid A \subseteq X\}\\
\semantics{\text no}(A) &=& \{X \subseteq U \mid  A \cap X = \emptyset\}\\
\semantics{n}(A) &=& \{X \subseteq U \mid \ \mid X \cap A \mid = n\}\\
\semantics{\text most}(A) &=& \{X \subseteq U \mid X \  \mbox{contains most} \ A\text{'s}\}\\
\semantics{\text few}(A) &=& \{X \subseteq U \mid  X \  \mbox{contains few} \ A\text{'s}\}
\end{eqnarray*}
 

Note that the interpretations of determiners are not quantifiers (in the sense of first order logic) yet. They become quantifiers when applied to a set. For instance, $\semantics{d}$ is not a quantifier, but $\semantics{d}(A)$ is. The interpretations of determiners satisfy a property referred to by \emph{living on} or \emph{conservativity}. This property says that a set $X$ belongs to  $\semantics{d}(A)$ iff  $(X \cap A) \in \semantics{d}(A)$. This property is sometimes described by saying that `the quantifier $\semantics{d}(A)$ lives on $A$'. 

\noindent
The interpretations of  expressions generated by the rules is obtained  by induction as follows:

\begin{itemize}
\item a noun phrase `NP' generated by the rule  `NP $\to$ Det N'  
\begin{eqnarray*}
\semantics{\mbox{Det N}} \ = \  \semantics{d}(\semantics{n})  
&\quad\text{where} \quad&  X \in \semantics{d}(\semantics{n}) \ \text{\bf iff} \ (X \cap \semantics{n}) \in \semantics{d}(\semantics{n})\\
&\quad \text{for} \quad&  d \in Det,  n \in N 
\end{eqnarray*}
\item a verb phrase `VP' generated by the rule `VP $\to$ V NP' 
\[
\semantics{\mbox{V NP}} = \left\{x \mid \semantics{np}\left(\{y \mid \semantics{v( x, y)}\}\right)\right\} \qquad
 \text{for} \quad  np \in NP,  v \in V  
\]
\item a sentence `S' generated by the rule `S $\to$ NP VP'
\[
\semantics{\text NP \ VP} = \semantics{vp}(\semantics{np}) \qquad
 \text{for} \quad  np \in NP,  vp \in VP
\]
\end{itemize}

The \emph{meaning} of a sentence   is  said to be  \emph{true} iff its semantic interpretation is non-empty and \emph{false} otherwise that is we have:
\begin{definition}
The meaning of a sentence in generalised quantifier theory is true  iff $\semantics{vp}(\semantics{np}) = 1$ and false otherwise. 
\end{definition}

As an example, consider the  meaning of a sentence with a quantified phrase at its subject position. This sentence has the form  `Det N VP'  and its meaning is defined as follows:
\[
\semantics{\mbox{Det N VP}} \ = \ \begin{cases} 1 & \semantics{vp} \in \semantics{\mbox{Det N}}  \quad \text{for} \ vp \in \text{VP}\\
0 & \mbox{other wise}
\end{cases}
\]
By the \emph{living on} property, the meaning of this sentence is true whenever  $\semantics{vp} \cap \semantics{n} \in \semantics{\mbox{Det N}}$. For instance, meaning of `some cats sneeze' is true whenever $\semantics{\text sneeze} \cap \semantics{\text men} \in \semantics{\mbox{some men}}$. That is, whenever the set of things that sneeze and are men is a non-empty set. Similarly, the meaning of the  sentence `five men sneeze' is true whenever the set of things that sneeze and are men has five elements and so on.  

As another example, consider the  meaning of a sentence with a quantified phrase at its object position. This sentence has the form  `NP V Det N'  and its meaning is defined as follows:
\[
\semantics{\mbox{NP V Det N}} \ = \ \begin{cases} 1 & \semantics{np} \in  \left\{x \mid \{y \mid \semantics{v(x,y)}\} \in \semantics{\mbox{Det N}}\right\}\quad \text{for} \ v \in \text{V}, np \in \text{NP} \\
0 & \mbox{otherwise}
\end{cases}
\] 
That is, the meaning of this sentence is true whenever  $\{y \mid \semantics{v(\semantics{np},y)}\}  \in \semantics{\mbox{Det N}}$, which by the \emph{living on} property is the case whenever  $\{y \mid \semantics{v(\semantics{np},y)}\}  \cap \semantics{n} \in \semantics{\mbox{Det N}}$, for $n$ in N. For instance, meaning of `John kissed some cats' is true whenever $\{y \mid \semantics{\text{kiss}(\semantics{John}, y)}\} \cap \semantics{\text cats} \in \semantics{\mbox{some cats}}$. That is, whenever, the set of things that are kissed by John and are cats is a non-empty set. Similarly, the sentence `John kissed five cats' is true whenever the set of things that are kissed by John and are cats has five elements and so on. 


\subsection{Type-Logical Grammar}
In this section we show how to syntactically analyse the sentences  of the previous fragment in a pregroup type logic. 


A pregroup algebra $P = (P, \leq, \cdot, (-)^r, (-)^l)$  is a partially ordered monoid where every element has a left and a right adjoint, that is, for every element $p \in P$, there are two elements $p^l, p^r \in P$, referred to as its left and right adjoint, and these satisfy the following four inequalities
\[
p \cdot p^r \leq 1 \leq p^r \cdot p\qquad
p^l \cdot p \leq 1 \leq p \cdot p^l
\]

A pregroup grammar over a vocabulary $\Sigma$  is a pregroup algebra $P_{\cal B}$,  freely generated over a  set of  atomic logical types ${\cal B}$,  together with a type-dictionary $\beta \subseteq \Sigma \times P_{\cal B}$, which assigns to each word of the vocabulary a  type from the pregroup.  We denote this grammar by the tuple ${\cal G}_{\Sigma} = (\beta, P_{\cal B})$. 

For the vocabulary and the  fragment of language described in the previous section, the set of atomic types are $\{m, n, s\} \in {\cal B}$, where $m$ is a noun phrase, $n$ is a noun, and $s$ is a declarative sentence.  The dictionary is as follows:

\begin{center}
\begin{tabular}{c|c}
 Words & Logical Types\\
\hline
 John, Mary, something, $\cdots$ & $m$\\
 cat, dog, man, $\cdots$ & $n$\\
 sneeze, sleep,$\cdots$ & $m^r \cdot s$\\
 love, kiss, $\cdots$ & $m^r \cdot s \cdot m^l$\\
 a, the, some, every, each, all, no, most, few, one, two, $\cdots$ & $m \cdot n^l$
\end{tabular}
\end{center}

We assign atomic types $m$ and $n$ to the words in the lexical categories NP and N. The type assigned to the words in VP is $m^r \cdot s$, this means that words in the  lexical category VP input an argument of type $m$ and they have to be to the right of that argument, then  output a sentence  of type $s$. The type assigned to the words in the lexicon item V is $m^r \cdot s \cdot m^l$, this means that these words input two arguments of type $m$ and they have to be to the right of one and to the left of the other, then output a sentence. Finally, the type assigned to the words in the lexicon item Det is $m\cdot n^l$, which means that these words input an argument of the type $n$ and they have to be to the left of that argument, then output a phrase of type $m$. 

The grammatical reductions of the language are modelled by the partial ordering of the pregroup grammar.  As an example, consider a sentence with a quantified phrase in its subject position, e.g. `some cats sneeze',   the grammatical reduction of this sentence is as follows:

\begin{center}
\begin{tabular}{cccc}
some & cats & sneeze &\\
$(m \cdot n^l)$ &$ \cdot n$ & $\cdot (m^r \cdot s)$ & $\leq m \cdot 1 \cdot (m^r \cdot s) = m \cdot (m^r \cdot s) \leq 1 \cdot s = s$
\end{tabular}
\end{center}

\noindent
Here, first `some' inputs `cats' and output a noun phrase of type $m$, then the verb inputs $m$ and outputs a sentence. As another example, consider a sentence with a quantified phrase in its object position, e.g. `John kissed some cats'  the grammatical reduction of this sentence is as follows:

\begin{center}
\begin{tabular}{ccccc}
John & kissed & some & cats&\\
$m$ & $\cdot (m^r \cdot s \cdot m^l)$ & $\cdot (m \cdot n^l)$ & $\cdot n$ & $\quad \leq \quad 1 \cdot (s \cdot m^l) \cdot m  \cdot 1 = (s \cdot m^l) \cdot m \leq  s\cdot 1 = s$
\end{tabular}
\end{center}

\noindent
Here, again first `some' inputs `cats' and outputs a noun phrase, at the same time the verb inputs `John' and outputs a verb phrase of type $s \cdot m^l$, which then  inputs the $m$ from the phrase `some cats' and outputs a sentence. 


Whereas the elements of lexical categories are syntactic, the types of a pregroup algebra have logical information in them.  This information comes from the rules of the grammar of the language;  in a lexical approach such information is encoded in the rules of the language. There are standard methods that transfer the lexical categories and rules of a language to logical type \cite{Penthus}. For the  fragment of the language considered in  this paper, we suffice to giving the following translation $t \colon S \to P_{\cal B}$ between the lexical categories and the pregroup grammar types:

\begin{center}
\begin{tabular}{c|c}
Syntactic Category & Logical Type\\
\hline
NP & $m$\\
N & $n$\\
VP &  $m^r \cdot s$\\
V  & $m^r \cdot s \cdot m^l$\\
Det  & $m \cdot n^l$
\end{tabular}
\end{center}

%\begin{center}
%\begin{tabular}{c|c}
%Generative Rule & Pregroup Partial Order\\
%\hline
%S $\to$  NP VP  \qquad&\qquad $m \cdot (m^r \cdot s) \leq 1 \cdot s = s$\\
%VP  $\to$  V NP \qquad&\qquad $(m^r \cdot s \cdot m^l) \cdot m \leq m^r \cdot s \cdot 1 = m^r \cdot s$\\
%NP $\to$  Det N \qquad& \qquad $(m \cdot n^l) \cdot n \leq m  \cdot 1 = m$
%\end{tabular}
%\end{center}
%


\subsection{Category Theoretic and Diagrammatic Definitions and Axioms}
This subsection briefly reviews compact closed
categories and Frobenius algebras. For a formal presentation, see
\cite{KellyLaplaza80,Kock72}.  A compact closed category, $\cC$, has objects $A, B$; morphisms $f \colon A
\to B$; a monoidal tensor $A \otimes B$ that has a unit $I$, that is we have $A \otimes I \cong I \otimes A \cong A$. Furthermore, for
each object $A$ there are two objects $A^r$ and $A^l$ and  the
following morphisms:
\begin{align*}
A \otimes A^r \stackrel{\epsilon_A^r} {\longrightarrow} \; &I
\stackrel{\eta_A^r}{\longrightarrow} A^r \otimes A \hspace{1cm}
A^l \otimes A \stackrel{\epsilon_A^l}{\longrightarrow} \; I
\stackrel{\eta_A^l}{\longrightarrow} A \otimes A^l\
\end{align*}
These morphisms satisfy the following equalities, sometimes
referred to as the \emph{yanking} equalities, where $1_A$ is the
identity morphism on object $A$:
\begin{align*}
& (1_A \otimes \epsilon_A^l) \circ (\eta_A^l \otimes 1_A)  = 1_A 
\hspace{1cm}
(\epsilon_A^r \otimes 1_A) \circ (1_A \otimes
  \eta_A^r)   = 1_A\\
& (\epsilon_A^l \otimes 1_A) \circ (1_{A^l} \otimes
  \eta_A^l) = 1_{A^l}  
    \hspace{1cm}
    (1_{A^r} \otimes \epsilon_A^r) \circ (\eta_A^r \otimes 1_{A^r}) = 1_{A^r}
\end{align*}
%
\noindent These express the fact the $A^l$ and $A^r$ are the left and right
adjoints, respectively, of $A$ in the 1-object bicategory whose
1-cells are objects of $\cC$.

%A pregroup \cite{Lambek08} is a partial order compact closed category,
%which we refer to as {\em Preg}. This means that the objects of {\em
%  Preg} are elements of a partially ordered monoid, and between any two
%objects $p,q \in \Preg$ there exists a morphism of type $p
%\to q$ iff $p \leq q$. Compositions of morphisms are obtained by
%transitivity and the identities by reflexivity of the partial
%order. The tensor of the category is the monoid multiplication, and
%the epsilon and eta maps are as follows:
%\begin{align*}
%\epsilon_p^r = p \cdot p^r \leq 1 &\hspace{1cm} \eta_p^r = 1 \leq p^r \cdot p \\
%\epsilon_p^l = p^l \cdot p \leq 1  &\hspace{1cm}  \eta_p^l = 1 \leq p \cdot p^l
%\end{align*}

A Frobenius algebra  in a   monoidal  category $({\cal
  C}, \otimes, I)$ is a tuple $(X,  \delta, \iota, \mu, \zeta)$ where,
for $X$ an object of ${\cal C}$, the triple $(X, \delta, \iota)$ is  an internal comonoid; 
i.e.~the following are  coassociative and counital  morphisms of ${\cal
  C}$:
\begin{align*}
\delta \colon X \to X \otimes X&\qquad& \iota \colon X \to I
\end{align*}
Moreover $(X, \mu, \zeta)$ is  an internal  monoid; i.e.~the following are  associative and unital  morphisms:
\begin{align*}
\mu \colon  X \otimes X \to X  &\qquad& \zeta \colon I \to X
\end{align*}
And finally the  $\delta$ and $\mu$ morphisms satisfy the
following \emph{Frobenius condition}:
\begin{align*}
\mbox{ $(\mu \otimes 1_X) \circ (1_X \otimes \delta) \ = \  \delta \circ \mu  \ = \  (1_X \otimes \mu) \circ (\delta \otimes 1_X)$}
\end{align*}
%MS
Informally, the  comultiplication $\delta$  dispatches the information contained in
one object into two objects, and the  multiplication $\mu$ unifies  the
information of two objects into one.

\medskip
\noindent
{\bf Pregroup Algebras.}
A pregroup algebra $P = (P, \leq, \cdot, (-)^l, (-)^r)$  is a compact closed category whose objects are the elements of the set $p \in P$ are the objects of the category and the partial ordering between the elements are the morphisms. That is,  for $p,q \in P$, we have that $p \to q$ is a morphism of the category iff $p \leq q$ in the partial order. The tensor product of the category is the monoid multiplication, whose unit is 1, and the adjoints of objects are the adjoints of the elements of the algebra.  The epsilon and eta morpshism are thus as follows:

\begin{align*}
p \cdot p^r \stackrel{\epsilon_p^r} {\longrightarrow} \; &1
\stackrel{\eta_p^r}{\longrightarrow} p^r \cdot p \hspace{1cm}
p^l \cdot p \stackrel{\epsilon_p^l}{\longrightarrow} \; 1
\stackrel{\eta_p^l}{\longrightarrow} p \cdot p^l\
\end{align*}
The yanking equalities directly follow from the preroup inequalities on the adjoints.  A pregroup with Frobenius structure on it becomes degenerate. To see this, suppose we have such an  algebra on the object $p$ of such a pregroup. Then the unit morphism of the internal comonoid of this algebra becomes the parietal ordering  $\iota \colon p \leq 1$; taking the right adjoints of both sides of this inequality will yield $1 = 1^r \leq p^r$, and by the multiplying both sides of this with $p$   we will obtain $p \leq p \cdot p^r$, which by adjunction results in $p \leq p \cdot p^r \leq 1$, hence we have $p \leq 1$ and also $1 \leq p$, thus $p$ must be equal to 1. That is, assuming that we have  a Frobenius algebra on an object will mean that that object is 1. 

\medskip
\noindent
{\bf Finite Dimensional Vector Spaces.}
These structures  together with  linear maps  form a compact
closed category, which we refer to as $\FdVect$.  Finite dimensional
vector spaces $V, W$ are objects of this category; linear maps $f
\colon V \to W$ are its morphisms with composition being the
composition of linear maps. The tensor product $V
\otimes W$ is the 
linear algebraic tensor product,
%tensor of the category, 
whose unit is the scalar
field of vector spaces; in our case this is the field of reals
$\mathbb{R}$.  Here, there is  a naturual
isomorphism $V \otimes W \cong W \otimes V$. As a result of the
symmetry of the tensor, the two adjoints reduce to one and we obtain the  isomorphism $V^l \cong V^r \cong V^*$, 
where $V^*$ is the dual space of $V$. When the
basis vectors of the vector spaces are fixed, it is further the case
that we have $V^* \cong V$.

%Elements of vector spaces, i.e. vectors, are
%represented by morphisms from the unit of tensor to their corresponding
%vector space; that is $\overrightarrow{v} \in V$ is represented by the
%morphism $\mathbb{R} \stackrel{\overrightarrow{v}}{\longrightarrow}V$; by linearity this morphism is uniquely defined when setting $1\mapsto \overrightarrow{v}$.
%
Given a basis $\{r_i\}_i$ for a vector space $V$, the epsilon maps are
given by the inner product extended by linearity; i.e. we have:
\[
\epsilon^l  =  \epsilon^r \colon   V \otimes V \to \mathbb{R} \quad \mbox{given by} \quad
\sum_{ij} c_{ij} \ \psi_i \otimes \phi_j  \quad \mapsto \quad \sum_{ij} c_{ij} \langle \psi_i \mid \phi_j \rangle\]
Similarly, eta maps   are defined as follows:
\[
\eta^l = \eta^r \colon   \mathbb{R} \to V \otimes V
\quad \mbox{given by} \quad 
1 \; \mapsto \; \sum_i r_i \otimes r_i
\]
Any vector space $V$ with a fixed basis
$\{\ov{v_i}\}_i$ has a Frobenius algebra over it, explicitly given as follows, where $\delta_{ij}$ is the Kronecker delta.
%\begin{align*}
%\delta :: \ov{v_i} \mapsto \ov{v_i} \otimes \ov{v_i} &\qquad& \iota:: \ov{v_i} \mapsto 1\\
%\mu:: \ov{v_i} \otimes \ov{v_i} \mapsto \ov{v_i} &\qquad& \zeta:: 1 \mapsto \ov{v_i}  
%\end{align*}
\begin{eqnarray*}\label{eq:frob}
\delta  \colon V \to V \otimes V  & \quad \mbox{given by} \quad&  \ov{v_i} \mapsto \ov{v_i} \otimes \ov{v_i} \\
\mu \colon V \otimes V \to V  &\quad \mbox{given by} \quad & \ov{v_i} \otimes \ov{v_j} \mapsto 
\delta_{ij}\ov{v_i} \\
  \iota \colon V \to \mathbb{R} & \quad \mbox{given by} \quad&  \ov{v_i} \mapsto 1 \\
 \zeta \colon \mathbb{R} \to V  &\quad \mbox{given by} \quad& 1 \mapsto   \sum_i  \ov{v_i}  
\end{eqnarray*}


%Frobenius algebras over vector spaces with orthonormal bases are
%moreover \emph{isometric} and \emph{commutative}. But in benefit of space, we do not provide these conditions here. 
%A
%commutative Frobenius Algebra satisfies the following two conditions
%for $ \sigma : X \otimes Y \to Y \otimes X$, the symmetry morphism of
%$({\cal C}, \otimes, I)$:
%\[
%\sigma \circ \delta = \delta \hspace{2cm} \mu \circ \sigma = \mu
%\]
%An isometric Frobenius Algebra is one that satisfies the following axiom:
%\[
%\mu \circ \delta = 1
%\]
%The vector spaces of distributional models have
%fixed orthonormal bases; hence they have isometric commutative
%Frobenius algebras over them.
%An object $X$ with a Frobenius algebra over it is called
%\emph{classical} in \cite{CoeckePaquettePavlovic09}. 


%As an example, take $V$ to be a two dimensional space with
%basis $\{\ov{v_1}, \ov{v_2}\}$; then the basis of $V \otimes V$ is
%$\{\ov{v_1} \otimes \ov{v_1}, \ov{v_1} \otimes \ov{v_2}, \ov{v_2}
%\otimes \ov{v_1}, \ov{v_2} \otimes \ov{v_2}\}$. For a vector $v = a
%\ov{v_1} + b \ov{n_2}$ in $V$ we have:
%\begingroup
%\setlength{\arraycolsep}{2pt}
%\[
%\delta (v) = \delta \left(\begin{array}{c} a\\b\end{array}\right)
% = 
% \left(\begin{array}{cc} a&0\\0&b\end{array}\right) = a \, \ov{v_1} \otimes \ov{v_1} + b \, \ov{v_2} \otimes \ov{v_2} 
%\]
%And for a matrix $w = a \, \ov{v_1} \otimes \ov{v_1} + b \, \ov{v_1}
%\otimes \ov{v_2} + c \, \ov{v_2} \otimes \ov{v_1} + d \, \ov{v_2}
%\otimes \ov{v_2}$ in $V \otimes V$, we have:
%\[
%\mu (w) = \mu \left(\begin{array}{cc} a&b\\c&d\end{array}\right) = 
%\left(\begin{array}{c} a\\d\end{array}\right)
%= a \, \ov{v_1} + d \, \ov{v_2}
%\]
%\endgroup

\medskip
\noindent
{\bf Relations}.
Another important example of a  compact closed category is
$\Rel$, the cateogry of sets and relations. Here, $\otimes$ is
cartesian product with the singleton set as its unit $I = \{\star\}$, and $^*$ is identity on objects. Closure reduces to the
fact that a relation between sets $A\times B$ and $C$ is equivalently a relation between $A$ and $B \times C$.   Given a set $S$ with elements $s_i, s_j \in S$,  the epsilon and eta maps are given as follows:

\begin{eqnarray*}
&\epsilon^l  =  \epsilon^r &\colon   S \times S \to \{\star\} \quad \mbox{given by} \quad
\{((s_i, s_j), \star) \mid s_i, s_j \in S, s_i = s_j \}\\
&\eta^l = \eta^r& \colon   \{\star\}  \to S \times S
\quad \mbox{given by} \quad 
\{(\star, (s_i, s_j)) \mid s_i, s_j \in S, s_i = s_j\}
\end{eqnarray*}



Every object in $\Rel$  has a
Frobenius algebra over it given by the diagonal and codiagonal
relations, as described below: 



\begin{eqnarray*}
&\delta &\colon   S \to S \times S \quad \mbox{given by} \quad
\{(s_i, (s_j, s_k)) \mid s_i, s_j, s_k \in S, s_i = s_j  = s_k\}\\
& \mu & \colon   S \times S \to S
\quad \mbox{given by} \quad 
\{(s_i, s_j), s_k) \mid s_i, s_j, s_k\in S, s_i = s_j= s_k\}\\
& \iota& \colon S \to  \{\star\}    \quad \mbox{given by} \quad \{(s_i, \star) \mid s_i \in S\}\\
&\zeta& \colon  \{\star\}  \to S  \quad \mbox{given by} \quad \{(\star, s_i) \mid s_i \in S\}
\end{eqnarray*}

For the details of verifying that for each of the two examples above,  the corresponding conditions hold see \cite{CoeckePaq}. 

\subsection{String Diagrams} 
\label{string}

The framework of compact closed categories and Frobenius algebras
comes with a complete diagrammatic calculus that visualises
derivations, and which also simplifies the
categorical and vector space computations. Morphisms are depicted by
boxes and objects by lines, representing their identity morphisms. For
instance a morphism $f \colon A \to B$, and an object $A$ with the
identity arrow $1_A \colon A \to A$, are depicted as follows:

\begin{center}
  \tikzfig{compact-diag}
\end{center}

Morphisms from $I$ to objects are depicted by triangles with strings emanating from them. In concrete categories, these morphisms represent  elements within the  objects. For instance, an element $a$ in $A$ is represented by the morphism $a: I \to A$ and depicted by a  triangle with one string emanating from it. The number of strings of such triangles depict the tensor rank of the element; for instance, the
diagrams for ${a} \in A, {a'} \in A \otimes B$, and ${a''}
\in A \otimes B \otimes C$ are as follows:

\begin{center}
  \tikzfig{compact-diag-triangle}  
\end{center} 


The tensor products of the objects and morphisms are depicted by
juxtaposing their diagrams side by side, whereas compositions of
morphisms are depicted by putting one on top of the other; for instance
the object $A \otimes B$, and the morphisms $f \otimes g$ and $f \circ
h$, for $f \colon A \to B, g \colon C \to D$, and $h \colon B \to C$,
are depicted as follows:

\begin{center}
  \tikzfig{compact-diag-tensor}
\end{center}

The $\epsilon$ maps are depicted by cups, $\eta$ maps by caps, and
yanking by their composition and straightening of the strings.  For
instance, the diagrams for $\epsilon^l \colon A^l \otimes A \to I$,
$\eta \colon I \to A\otimes A^l$ and $(\epsilon^l \otimes 1_A) \circ
(1_A \otimes \eta^l) = 1_A$ are as follows:

\begin{center}
  \tikzfig{compact-cap-cup}
  \qquad
    \tikzfig{compact-yank}
\end{center}

 
As for Frobenius algebras, the diagrams for the  monoid and  comonoid 
morphisms are as follows:

\begin{center}
\tikzfig{comp-alg-coalg}
\end{center} 
 
\noindent
with the Frobenius condition being depicted as:

\begin{center}
\tikzfig{equation}
\end{center} 

%\noindent The commutativity conditions are depicted as follows:
%
%\begin{center}
%\tikzfig{comm}
%\end{center} 
%
%\noindent The isometry condition is depicted as follows:
%
%\begin{center}
%\tikzfig{isom}
%\end{center} 
%

\noindent
The defining axioms   guarantee that any picture depicting a
Frobenius computation can be reduced to a normal form that only
depends on the number of input and output strings of the nodes,
independent of the topology. 
These normal forms can be simplified to so-called `spiders': 
%This property, known as the \emph{spider  equality}, is depicted as follows:

\[
\tikzfig{spider}
\]


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "Quant"
%%% End: 
